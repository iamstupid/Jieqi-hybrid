import random
import collections
import jieqi_game

def run_direct_determinization_game():
    """
    Initializes a single determinized game using the direct constructor
    and plays a random game within that reality for up to 30 plies.
    """
    # 1. Create the initial game state
    # This history object is the canonical, non-randomized starting point.
    history = jieqi_game.PositionHistory()
    start_board = jieqi_game.ChessBoard.hStartposBoard
    history.Reset(start_board, 0, 0)

    # 2. Construct a DeterminizedGame directly from the history.
    # The py::keep_alive policy ensures 'history' is not garbage-collected
    # while 'det_game' is in use.
    det_game = jieqi_game.DeterminizedGame(history)

    # Manually call determinize() to populate the board with a random sample
    # of the hidden pieces.
    det_game.determinize()
    print("--- Starting a game in a single, directly-created determinization ---")
    print("-" * 60)

    # Helper for printing piece names
    piece_names = {
        'p': "Pawn", 'c': "Cannon", 'r': "Rook",
        'a': "Advisor", 'b': "Bishop", 'n': "Knight"
    }

    # 3. Main game loop within the single determinization
    for ply in range(30):
        # Get state from our determinized game instance.
        det_history = det_game.get_position_history()
        current_pos = det_history.Last()
        board = current_pos.GetBoard()

        # --- Print Information ---
        player = "Black" if det_history.IsBlackToMove() else "White"
        print(f"Ply {ply + 1: >2} ({player: <5})")

        # Get the sampled dark pieces for the current player.
        our_dark_pieces_info = current_pos.our_dark()
        sampled_pieces_str = our_dark_pieces_info.pieces
        piece_counts = collections.Counter(sampled_pieces_str)
        print(f"  Dark Pieces ({our_dark_pieces_info.nleft} left): ", end="")
        
        sorted_pieces = sorted(piece_counts.items())
        print(", ".join([f"{piece_names.get(p, p)}: {c}" for p, c in sorted_pieces]))

        # --- Advance the Game ---
        legal_moves = board.GenerateLegalMoves()
        if not legal_moves:
            print(f"\nGame over at ply {ply}. No legal moves available.")
            break

        chosen_move = random.choice(legal_moves)
        print(f"  Move Played: {chosen_move.as_string()}")
        print("-" * 60)

        # Use the append method on the DeterminizedGame object directly.
        det_game.append(chosen_move)
        print(f"  Board:\n {det_game.get_position_history().Last().DebugString()}")

        result = det_history.ComputeGameResult()
        if result != jieqi_game.GameResult.UNDECIDED:
            print(f"\nGame over. Result: {result}")
            break

    # 4. Print final state
    final_history = det_game.get_position_history()
    print("--- Game Finished ---")
    print(f"Final Ply Count: {final_history.GetLength() - 1}")
    print(f"  Board:\n {det_game.get_position_history().Last().DebugString()}")
    print(f"Final FEN: {jieqi_game.GetFen(final_history.Last())}")


if __name__ == "__main__":
    # Initialize C++ magic bitboards.
    jieqi_game.InitializeMagicBitboards()
    run_direct_determinization_game()


import numpy as np
import jieqi_game

def simulate_nn_evaluation(nn_input_batch: np.ndarray):
    """
    Mocks the behavior of a neural network.

    Takes a batch of encoded game states and returns random policy and value tensors.
    """
    # The batch size can vary depending on the number of leaf nodes found
    batch_size = nn_input_batch.shape[0]
    
    # The number of possible moves in the policy head.
    # Move::as_nn_index() produces an index from 0 to 2061.
    num_possible_moves = 2062

    # 1. Mock Policy Head Output
    # Create random logits and apply softmax to get a probability distribution.
    dummy_policy_logits = np.random.rand(batch_size, num_possible_moves).astype(np.float32)
    policy_exp = np.exp(dummy_policy_logits)
    policy_output = policy_exp / np.sum(policy_exp, axis=1, keepdims=True)

    # 2. Mock Value Head Output
    # Create random logits for (Loss, Draw, Win) and apply softmax.
    dummy_value_logits = np.random.randn(batch_size, 3).astype(np.float32)
    value_exp = np.exp(dummy_value_logits)
    value_output = value_exp / np.sum(value_exp, axis=1, keepdims=True)
    
    # The C++ side expects flattened arrays for the data spans.
    return policy_output.flatten(), value_output.flatten()


def run_mcts_test():
    """
    Sets up a position, runs an MCTS search with a mock NN,
    and prints the resulting move evaluations.
    """
    # 1. Set up a non-trivial game position
    history = jieqi_game.PositionHistory()
    history.Reset(jieqi_game.ChessBoard.hStartposBoard, 0, 0)

    print("--- Testing MCTS on the following position ---")
    print(f"FEN: {jieqi_game.GetFen(history.Last())}")
    print(f"Player to move: {'Black' if history.IsBlackToMove() else 'White'}")
    print("-" * 50)

    # 2. Initialize MCTS search
    batch_size = 8
    num_simulations = 400
    mcts = jieqi_game.MCTS(history, batch_size=batch_size, cpuct=1.5) #
    
    # 3. Main search loop
    print(f"Running {num_simulations} simulations with batch size {batch_size}...")
    for _ in range(num_simulations // batch_size):
        # Get a batch of encoded leaf nodes for the NN.
        nn_input = mcts.run_search_batch()

        # If the batch is empty, the tree is fully explored.
        if nn_input.size == 0:
            print("Search tree fully explored, stopping early.")
            break
            
        # Get mock NN predictions.
        mock_policy_output, mock_value_output = simulate_nn_evaluation(nn_input)
        
        # Apply the mock evaluations to the tree.
        mcts.apply_evaluations(mock_policy_output, mock_value_output)

    print("Search complete.")
    print("-" * 50)

    # 4. Print results
    print("--- MCTS Root Move Evaluations ---")
    
    # Get the detailed search statistics for each move from the root.
    move_evals = mcts.get_root_move_evaluations()
    
    # Sort by visit count in descending order for readability
    sorted_evals = sorted(move_evals, key=lambda x: x.visit_count, reverse=True)

    # Print a formatted table header
    print(f"{'Move':<10} | {'Visits':>8} | {'Policy':>8} | {'Win%':>7} | {'Draw%':>7} | {'Loss%':>7}")
    print(f"{'-'*10:s}-+-{'-'*8:s}-+-{'-'*8:s}-+-{'-'*7:s}-+-{'-'*7:s}-+-{'-'*7:s}")

    for evaluation in sorted_evals:
        win_pct = evaluation.win_prob * 100
        draw_pct = evaluation.draw_prob * 100
        loss_pct = evaluation.loss_prob * 100
        
        print(f"{evaluation.move.as_string():<10} | {evaluation.visit_count:>8d} | {evaluation.policy_prior:8.4f} | "
              f"{win_pct:6.2f}% | {draw_pct:6.2f}% | {loss_pct:6.2f}%")

    # Get and print the best move found.
    best_move = mcts.get_best_move()
    print("-" * 50)
    print(f"üèÜ Best Move according to search: {best_move.as_string()}")


if __name__ == "__main__":
    jieqi_game.InitializeMagicBitboards()
    run_mcts_test()


import factory


factory.EXAMPLE_CONFIGS


import jieqi_game as game


model_config = factory.EXAMPLE_CONFIGS["small"].copy()


model_config


model_config["policy_index_array"]=numpy(game.K_ATTN_POLICY_MAP)


import numpy as np


model_config["policy_index_array"]=list(game.K_ATTN_POLICY_MAP)


model_config


model = factory.create_leela_model(model_config)
print(f"Created model with {sum(p.numel() for p in model.parameters())} parameters")


model.policy_head.policy_filter.policy_index_array



